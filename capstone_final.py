# -*- coding: utf-8 -*-
"""CAPSTONE_Final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IKOWPpuQ5jdOfwQ6JMfHye1UbWOtkuk6

**Data Preparation**

Converting Log Files to CSV and save on folder
"""

import re
import pandas as pd
import os

# Directory containing your .nmfs log files
log_directory = r"/content/drive/MyDrive/CAPSTONE_DATA/LOGS"
output_csv = r"/content/drive/MyDrive/CAPSTONE_DATA/LOGS/CombineFinal1234.CSV"  # Output CSV file path

# Initialize lists to store log information
log_info = []

# Function to calculate file size in KB
def get_file_size(file_path):
    return os.path.getsize(file_path) / 1024  # Convert bytes to KB

# Iterate through all .nmfs files in the specified directory
for filename in os.listdir(log_directory):
    if filename.endswith(".nmfs"):
        file_path = os.path.join(log_directory, filename)

        # Determine label based on the file suffix and file size
        suffix = filename[-7:]  # Assuming the suffix is the last 8 characters
        file_size_kb = get_file_size(file_path)

        if suffix == ".1.nmfs" and file_size_kb <= 1000 and file_size_kb >= 100:
            label = "MOBILE_ORIGINATING"
        elif suffix == ".2.nmfs" and file_size_kb <= 1000 and file_size_kb >= 100:
            label = "MOBILE_TERMINATING"
        elif suffix == ".1.nmfs" and file_size_kb >= 1000 and file_size_kb <= 7000:
            label = "4G_DATA"
        elif suffix == ".2.nmfs" and file_size_kb >= 1000 and file_size_kb <= 7000:
            label = "3G_DATA"
        else:
            label = "UNKNOWN"  # Handle any other cases

        # Extract date and time from the log name (adjust this part based on your log naming convention)
        log_name = os.path.splitext(filename)[0]  # Remove file extension
        log_date = log_name[0:7]  # Assuming the date is the first 6 characters
        log_time = log_name[8:12]  # Assuming the time is the next 6 characters
        Slice=log_name[15:16]

        log_info.append([filename, label, log_date, log_time, file_size_kb, Slice])

# Create a DataFrame from the log information
log_df = pd.DataFrame(log_info, columns=['Log Name', 'Label', 'Date', 'Time', 'Size_Of_Log_KB', 'Slice'])

# Export the log information to a CSV file, overwriting if it already exists
log_df.to_csv(output_csv, index=False, mode='w')

print("Log information exported to CSV file:", output_csv)

"""**Importing CSV file into Pandas**


"""

import pandas as pd

# Specify the file path
file_path = '/content/drive/MyDrive/CAPSTONE_DATA/LOGS/CombineFinal1234.CSV'  # Replace with the actual file path

# Use pandas to read the data into a DataFrame
df = pd.read_csv(file_path)  # For CSV files

# If you have an Excel file, you can use read_excel:
# df = pd.read_excel(file_path)

# Now, you can work with your data in the 'df' DataFrame
df

df.head()

df.Size_Of_Log_KB

df.describe()

df.Size_Of_Log_KB

average_size_of_log_kb = df.Size_Of_Log_KB.mean()
print( "Average Value of Size of log in KB =", average_size_of_log_kb)

maximum_size_of_Log_kb=df.Size_Of_Log_KB.max()
print( "Max Value of Size of log in KB =", maximum_size_of_Log_kb)

Minimum_size_of_Log_kb=df.Size_Of_Log_KB.min()
print( "Minimun Value of Size of log in KB =", Minimum_size_of_Log_kb)

Total_size_of_Log_kb=df.Size_Of_Log_KB.sum()
print( "Total Size of log in KB=", Total_size_of_Log_kb)

df.isnull()

# Count the number of null values in each column
null_counts = df.isnull().sum()
null_counts

df.dtypes

df.isna().sum() #Checking null values

count=0
for i in df.isnull().sum(axis=1):
    if i>0:
        count=count+1
print('Total number of rows with missing values is ', count)
print('since it is only',round((count/len(df.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')

df.dropna(inplace=True)

df.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)

# Class distribution
class_distribution = df['Label'].value_counts()
class_distribution

#import ydata_profiling
import seaborn as sns
import matplotlib.pyplot as plt
# Pairplot for visualizing relationships between features
sns.pairplot(df, hue='Label', diag_kind='kde')
plt.show()#

import seaborn as sns
import matplotlib.pyplot as plt



def draw_histograms(dataframe, features, rows, cols):
    fig=plt.figure(figsize=(20,20))
    for i, feature in enumerate(features):
        ax=fig.add_subplot(rows,cols,i+1)
        dataframe[feature].hist(bins=20,ax=ax,facecolor='midnightblue')
        ax.set_title(feature+" Distribution",color='DarkRed')

    fig.tight_layout()
    plt.show()
draw_histograms(df, df.columns,6,3)

import matplotlib.pyplot as plt
import seaborn as sn
sn.countplot(x='Time', data=df)

sn.countplot(x='Label', data=df)

class_distribution = df['Label'].value_counts()
class_distribution



sn.pairplot(data=df)

# Define your features (X) and target (y)
X = df[['Size_Of_Log_KB', 'Slice']]
y = df['Label']
y.count()
class_distribution = df['Label'].value_counts()
class_distribution

# Correlation heatmap
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

# Correlation heatmap
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

import plotly.express as px

x = df['Label'].value_counts()
colors = ['#800080', '#0000A0']

fig = px.pie(names=x.index, values=x, title="Types of Logs")
fig.update_traces(marker=dict(colors=colors, line=dict(color='#001000', width=2)))

fig.show()

p = df.hist(figsize = (20,20))

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()
df['Label'] = encoder.fit_transform(df['Label'])
df

# Correlation heatmap
correlation_matrix = df.corr()
plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.show()

df.dtypes

# Define your features (X) and target (y)
X = df[['Size_Of_Log_KB', 'Slice']]
y = df['Label']
class_distribution = df['Label'].value_counts()
y, class_distribution

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming you have already loaded your data into a Pandas DataFrame called 'df'
# 'Label', 'size of log', and 'Slice' are columns in your DataFrame



# Split your data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train a classification model (Random Forest classifier in this example)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = classifier.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have already loaded your data into a Pandas DataFrame called 'df'
# 'Label', 'Size_Of_Log_KB', and 'Slice' are columns in your DataFrame


# Split your data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)

# Create and train a classification model (Random Forest classifier in this example)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = classifier.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print accuracy and classification report
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using seaborn and matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

#Overdsampling of images and labels to eliminate bias issue
from imblearn.over_sampling import SMOTE
sampler = SMOTE()
x_smote,y_smote = sampler.fit_resample (X ,y)

pd.Series(y_smote).value_counts()

import seaborn as sns  # Import Seaborn and use 'sns' as an alias

# Assuming you have already imported your DataFrame 'df'
sns.countplot(x=y_smote, data=df)  # Use 'sns' to call the countplot function

import plotly.express as px

x = y_smote.value_counts()
colors = ['#800080', '#0000A0']

fig = px.pie(names=x.index, values=x, title="Types of Logs")
fig.update_traces(marker=dict(colors=colors, line=dict(color='#001000', width=2)))

fig.show()

from sklearn.impute import SimpleImputer

# Create an imputer object
imputer = SimpleImputer(strategy='mean')  # You can choose other strategies as well

# Fit and transform the imputer on your data
X = imputer.fit_transform(X)

from sklearn.model_selection import train_test_split
from sklearn.experimental import enable_hist_gradient_boosting
from sklearn.ensemble import HistGradientBoostingClassifier

# Split your data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.2, random_state=42)

# Create and train a classification model using HistGradientBoostingClassifier
classifier = HistGradientBoostingClassifier(random_state=42)
classifier.fit(X_train, y_train)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report

# Assuming you have already loaded your data into a Pandas DataFrame called 'df'
# 'Label', 'size of log', and 'Slice' are columns in your DataFrame



# Split your data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.2, random_state=42)

# Create and train a classification model (Random Forest classifier in this example)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = classifier.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

print("Accuracy:", accuracy)
print("Classification Report:\n", report)

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming you have already loaded your data into a Pandas DataFrame called 'df'
# 'Label', 'Size_Of_Log_KB', and 'Slice' are columns in your DataFrame


# Split your data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.2, random_state=42)

# Create and train a classification model (Random Forest classifier in this example)
classifier = RandomForestClassifier(n_estimators=100, random_state=42)
classifier.fit(X_train, y_train)

# Make predictions on the test set
y_pred = classifier.predict(X_test)

# Evaluate the model's performance
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# Print accuracy and classification report
print("Accuracy:", accuracy)
print("Classification Report:\n", report)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Plot the confusion matrix using seaborn and matplotlib
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

# Define your features (X) and target (y)
X = df[['Size_Of_Log_KB', 'Slice']]
y = df['Label']

# Define your features (X) and target (y)
X = df[['Size_Of_Log_KB', 'Slice']]
y = df['Label']



print(y_smote, "\n", x_smote)

#importing train_test_split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split (x_smote, y_smote,test_size=1/3,random_state=42, stratify=y_smote)

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score
# Evaluate the model's performance
# Make predictions on the test set
y_pred = classifier.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
confusion = confusion_matrix(y_test, y_pred)
precision = precision_score(y_test, y_pred, average=None)

recall = recall_score(y_test, y_pred, average=None)

#importing train_test_split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=1/3,random_state=42, stratify=y_smote)

from sklearn.neighbors import KNeighborsClassifier


test_scores = []
train_scores = []

for i in range(1,15):

    knn = KNeighborsClassifier(i)
    knn.fit(X_train,y_train)

    train_scores.append(knn.score(X_train,y_train))
    test_scores.append(knn.score(X_test,y_test))

## score that comes from testing on the same datapoints that were used for training
max_train_score = max(train_scores)
train_scores_ind = [i for i, v in enumerate(train_scores) if v == max_train_score]
print('Max train score {} % and k = {}'.format(max_train_score*100,list(map(lambda x: x+1, train_scores_ind))))

## score that comes from testing on the datapoints that were split in the beginning to be used for testing solely
max_test_score = max(test_scores)
test_scores_ind = [i for i, v in enumerate(test_scores) if v == max_test_score]
print('Max test score {} % and k = {}'.format(max_test_score*100,list(map(lambda x: x+1, test_scores_ind))))

plt.figure(figsize=(12,5))

# Create a DataFrame from the range and scores
data = pd.DataFrame({'x': range(1, 15), 'train_scores': train_scores, 'test_scores': test_scores})

# Use sns.lineplot to plot the data
p = sns.lineplot(data=data, x='x', y='train_scores', marker='*', label='Train Score')
p = sns.lineplot(data=data, x='x', y='test_scores', marker='o', label='Test Score')

#Setup a knn classifier with k neighbors
knn = KNeighborsClassifier(11)

knn.fit(X_train,y_train)
knn.score(X_test,y_test)

#Overdsampling of images and labels to eliminate bias issue
from imblearn.over_sampling import SMOTE
sampler = SMOTE()
x_smote,y_smote = sampler.fit_resample (X ,y)

pd.Series(y_smote).value_counts()

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.utils import to_categorical
y = to_categorical(y)
X_train, X_test, y_train, y_test = train_test_split(x_smote, y_smote, test_size=0.2, random_state=42)

# Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#importing train_test_split
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(x_smote,y_smote,test_size=1/3,random_state=42, stratify=y_smote)

# Feature Scaling because yes we don't want one independent variable dominating the other and it makes computations easy
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

# sequential model to initialise our ann and dense module to build the layers
from keras.models import Sequential
from keras.layers import Dense

from keras.models import Sequential
from keras.layers import Dense

# Assuming X_train and y_train are your training data and labels
# X_train should have shape (number_of_samples, 2) for two input features

# Define the model
model = Sequential()

# Add the input layer with 2 input features
model.add(Dense(units=8, input_dim=2, activation='relu'))

# Add one hidden layer
model.add(Dense(units=6, activation='relu'))

# Add the output layer with 4 output classes and softmax activation
model.add(Dense(units=4, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Assuming y_train is one-hot encoded (using to_categorical in Keras)
# If not, you can use the following to one-hot encode your labels:
# from keras.utils import to_categorical
# y_train = to_categorical(y_train)

# Fit the model to the training data
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# Evaluate the model on the training data
train_loss, train_accuracy = model.evaluate(X_train, y_train)
print('Training Accuracy:', train_accuracy)

# Assuming X_test and y_test are your test data and labels
# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print('Test Accuracy:', test_accuracy)

# Display the summary of the model
model.summary()

# ...

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=10)

# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)

# Print the evaluation results
print(f'Test Loss: {test_loss}')
print(f'Test Accuracy: {test_accuracy}')

# Additionally, if you want to make predictions on new data, you can use the following code:

# Make predictions on new data (X_new)
X_new = pd.DataFrame({'Size of Logs': [0.0, 5.0], 'Slice': [0.0, 3.0]})
#X_new = scaler.transform(X_new)  # Standardize new data if necessary
predictions = model.predict(X_test)

from sklearn.preprocessing import LabelEncoder

# Assuming you have a label_encoder object
label_encoder = LabelEncoder()
label_encoder.fit(y_train)  # Fit the label encoder on your training labels (y_train)

# Then, you can use it for transformation
predictions = model.predict(X_test)
predicted_labels = label_encoder.inverse_transform(predictions.argmax(axis=1))
print('Predicted Labels:', predicted_labels)

from keras.models import Sequential
from keras.layers import Dense

# Assuming X_train and y_train are your training data and labels
# X_train should have shape (number_of_samples, 2) for two input features

# Define the model
model = Sequential()

# Add the input layer with 2 input features
model.add(Dense(units=8, input_dim=2, activation='relu'))

# Add one hidden layer
model.add(Dense(units=6, activation='relu'))

# Add the output layer with 4 output classes and softmax activation
model.add(Dense(units=4, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display the summary of the model
model.summary()

# Assuming y_train is one-hot encoded (using to_categorical in Keras)
# If not, you can use the following to one-hot encode your labels:
# from keras.utils import to_categorical
# y_train = to_categorical(y_train)

# Fit the model to the training data
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# Evaluate the model on the training data
train_loss, train_accuracy = model.evaluate(X_train, y_train)
print('Training Accuracy:', train_accuracy)

# Assuming X_test and y_test are your test data and labels
# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print('Test Accuracy:', test_accuracy)

from keras.models import Sequential
from keras.layers import Dense

# Assuming X_train and y_train are your training data and labels
# X_train should have shape (number_of_samples, 2) for two input features

# Define the model
model = Sequential()

# Add the input layer with 2 input features
model.add(Dense(units=8, input_dim=2, activation='relu'))

# Add one hidden layer
model.add(Dense(units=6, activation='relu'))

# Add the output layer with 4 output classes and softmax activation
model.add(Dense(units=4, activation='softmax'))

# Compile the model
model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Display the summary of the model
model.summary()

# Assuming y_train is one-hot encoded (using to_categorical in Keras)
# If not, you can use the following to one-hot encode your labels:
# from keras.utils import to_categorical
# y_train = to_categorical(y_train)

# Fit the model to the training data
model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)

# Evaluate the model on the training data
train_loss, train_accuracy = model.evaluate(X_train, y_train)
print('Training Accuracy:', train_accuracy)

# Assuming X_test and y_test are your test data and labels
# Evaluate the model on the test data
test_loss, test_accuracy = model.evaluate(X_test, y_test)
print('Test Accuracy:', test_accuracy)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Assuming you've already trained your model and have test data predictions
# If not, you can use the following code to make predictions:
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=-1)  # Convert one-hot encoded predictions to class labels

# Create the confusion matrix
cm = confusion_matrix(y_test, y_pred)  # Replace y_pred with your model's predictions

# Define class labels if you have them
class_labels = ['Class1', 'Class2', 'Class3', 'Class4']  # Replace with your class labels

# Create a figure for the confusion matrix plot
#fig, ax = plt.subplots(figsize=(8, 6))

# Plot the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)
disp.plot(cmap=plt.cm.Blues, values_format='d')

# Add labels and title
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.title('Confusion Matrix')

# Show the plot
plt.show()

import numpy as np

# Make predictions using the model
y_pred = model.predict(X_test)

# Extract probability scores for each class
y_scores = y_pred[:, 0:4]  # Replace positive_class with the class of interest
print(y_scores)

import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from sklearn.preprocessing import label_binarize

# Assuming you have a trained multiclass classification model stored in 'model'
# Replace 'model' with your actual model

# Make predictions using the model
y_pred = model.predict(X_test)

# Binarize the true labels (y_test) to one-hot encoding
y_test_bin = label_binarize(y_test, classes=[0, 1, 2, 3])  # Replace [0, 1, 2, 3] with your class labels

# Initialize lists to store FPR, TPR, and AUC for each class
fpr = dict()
tpr = dict()
roc_auc = dict()

# For each class, calculate and plot the ROC curve
for i in range(4):  # Assuming you have 4 classes
    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_pred[:, i])
    roc_auc[i] = auc(fpr[i], tpr[i])
    plt.plot(fpr[i], tpr[i], label=f'Class {i} (AUC = {roc_auc[i]:.2f})')


# Plot settings
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC)')
plt.legend(loc='lower right')
plt.show()

from sklearn.metrics import classification_report

# Assuming you have a trained classification model and test data
# Replace 'model' with your actual model and 'X_test' and 'y_test' with your test data

# Make predictions using the model
y_pred = model.predict(X_test)

# Convert predicted probabilities to class labels (if necessary)
y_pred = y_pred.argmax(axis=1)  # Convert predicted probabilities to class labels

# Generate the classification report
report = classification_report(y_test, y_pred)

# Print the report
print(report)

from sklearn.metrics import classification_report, confusion_matrix

# Assuming y_test is your true labels and y_pred is the predicted labels from your model

# Convert predicted labels to one-hot encoding (if not already)
# y_pred = model.predict(X_test)
# y_pred_classes = np.argmax(y_pred, axis=1)

# If your labels are one-hot encoded, you can convert them back to integers
# y_test_classes = np.argmax(y_test, axis=1)

# Get the classification report
report = classification_report(y_test, y_pred)

# Print the report
print("Classification Report:\n", report)

# Optionally, you can also print the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:\n", conf_matrix)

#import classification_report
from sklearn.metrics import classification_report

print(classification_report(y_test,y_pred))

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Assuming you have true labels 'y_true' and predicted labels 'y_pred'

# Compute the confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)

class_names = ["class_0", "Class 1", "Class 2", "Class 3"]
# Display the confusion matrix using a heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

"""**Prediction of Model**"""

import numpy as np
Device = input("Enter The Device:")

Size=input('Enter The Size of log in KB :')

x=(Size, Device)

import numpy as np
input=np.asarray(x)
input_reshape=input.reshape(1,-1)

# Make predictions using the RandomForestClassifier
prediction = classifier.predict(input_reshape)
print(prediction)

if prediction[0] == 0:
    print('The Output Class is 3G Data')
elif prediction[0] == 1:
    print('The Output Class is 4G Data')
elif prediction[0] == 2:
    print('The Output Class is Mobile Originating')
else:
    print('The Output Class is Mobile Terminating')

"""**Saving the traine model**"""

import pickle
filename = 'Log_Model2.pkl'
pickle.dump(classifier, open(filename, 'wb'))

import joblib

# Save the model to a .pkl file
joblib.dump(classifier, 'your_model.pkl')



import pickle
with open('your_model.pkl', 'wb') as file:
    pickle.dump(model, file)